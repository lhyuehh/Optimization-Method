{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Homework 2 \n","\n","### Template for Gradient Descent Using Python and AD (automatic differentiation)\n","\n","### Before start, make sure you have configured python environment and have PyTorch, Numpy package installed."]},{"cell_type":"markdown","metadata":{},"source":["## Some hints\n","\n","1. Be sure to clear gradient at the end of each iteration\n","2. Use backtracking line search for choosing step size\n","3. Check gradient norm as the stopping criterion\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-10-19T07:18:13.369088Z","iopub.status.busy":"2023-10-19T07:18:13.368670Z","iopub.status.idle":"2023-10-19T07:18:13.374932Z","shell.execute_reply":"2023-10-19T07:18:13.373979Z","shell.execute_reply.started":"2023-10-19T07:18:13.369056Z"},"trusted":true},"outputs":[],"source":["def my_GD(data, params, loss, eps = 0.1, max_iter = 1e3):\n","    # define the constant for backtracking line search\n","    ls_alpha = 0.01\n","    ls_beta = 0.5\n","    \n","    for i in range(int(max_iter)):\n","        ## calculate the loss\n","        \n","        ## calculate the current gradient (on params)\n","        \n","        ## if the gradient is small enough, stop and return\n","        \n","        ## if not, perform backtracking line search and update the parameters\n","        \n","        ## clear the gradient\n","        params.grad.data.zero_()\n","        \n","    ## return the parameters\n","        "]},{"cell_type":"markdown","metadata":{},"source":["## Rosenbrock 函数优化\n","\n","$$\n","\\min_{\\boldsymbol{x}\\in\\mathbb{R}^2} (1-x_1)^2 + 100(x_2 - x_1^2)^2.\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def loss(data, params):\n","    ## \n","\n","## choose the initial parameters\n","params = torch.tensor(####, requires_grad=True)\n","\n","## run the gradient descent algorithm\n","## there is no data in this objective function, so we just pass in a dummy variable _\n","beta_est, obj_all, params_all = my_GD(_, params, loss)\n","\n","## plot the objective function\n","import matplotlib.pyplot as plt\n","x = np.linspace(-20, 20, 1000)\n","y = np.linspace(-20, 20, 1000)\n","\n","X, Y = np.meshgrid(x, y)\n","Z = 5 * X * X + 0.5 * Y * Y\n","plt.contour(X, Y, Z, colors='black')\n","params_all = np.array(params_all)\n","plt.plot(params_all[:,0],params_all[:,1], '-o')\n","plt.xlim(-20, 20)\n","plt.ylim(-20, 20)"]},{"cell_type":"markdown","metadata":{},"source":["## Beale 函数\n","$$\n","\\min_{\\boldsymbol{x}\\in\\mathbb{R}^2} (1.5 - x_1 + x_1 x_2)^2 + (2.25 - x_1 + x_1 x_2^2)^2 + (2.625 - x_1 + x_1 x_2^3)^2.\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## similar to the previous example"]},{"cell_type":"markdown","metadata":{},"source":["## 线性回归问题（已给定模拟数据）\n","$$\n","\\min_{\\beta\\in\\mathbb{R}^p} \\frac{1}{2n}\\sum_{i=1}^n(y_i-\\beta^\\top x_i)^2.\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Define the loss function of linear regression\n","def loss(data, beta):\n","    ## something\n","\n","## Example of least square regression\n","import torch\n","import numpy as np\n","\n","n = 100\n","p = 15\n","\n","# Generate data.\n","torch.manual_seed(123)\n","X = torch.randn(n, p)\n","y = torch.randn(n)\n","data = {'X': X, 'y': y}\n","\n","## initialize the parameter variables\n","## and fit the data to the model\n","params = torch.ones(p, requires_grad=True)\n","beta_est, obj_all, _ = my_GD(data, params, loss)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Check results\n","import matplotlib.pyplot as plt\n","\n","beta_explicit = torch.inverse(X.T@X)@X.T@y\n","plt.plot(obj_all)\n","plt.axhline(y = loss(data, beta_explicit))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Compare with explicit solution\n","torch.set_printoptions(sci_mode=False, precision=5)\n","print(beta_est)\n","beta_explicit"]},{"cell_type":"markdown","metadata":{},"source":["## 逻辑回归问题（已给定模拟数据）\n","$$\n","\\min_{\\beta\\in\\mathbb{R}^p} \\frac{1}{n}\\sum_{i=1}^n\\log (1+\\exp(-y_i\\beta^\\top x_i)).\n","$$"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-10-19T08:08:09.913018Z","iopub.status.busy":"2023-10-19T08:08:09.912626Z","iopub.status.idle":"2023-10-19T08:08:09.929572Z","shell.execute_reply":"2023-10-19T08:08:09.928088Z","shell.execute_reply.started":"2023-10-19T08:08:09.912990Z"},"trusted":true},"outputs":[],"source":["## Define the loss function of logistic regression\n","def logistic_loss(data, beta):\n","    # something\n","\n","## Example of least square regression\n","import torch\n","import numpy as np\n","\n","n = 100\n","p = 15\n","\n","# Generate data.\n","torch.manual_seed(123)\n","X = torch.randn(n, p)\n","beta_true = torch.randn(p)\n","y = torch.distributions.Bernoulli(logits = X @ beta_true).sample()\n","data = {'X': X, 'y': y}\n","\n","## initialize the parameter variables\n","## and fit the data to the model\n","params = torch.ones(p, requires_grad=True)\n","beta_est, obj_all, _ = my_GD(data, params, loss)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
